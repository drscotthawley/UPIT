{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANILLA model\n",
    "\n",
    "> Defines the GANILLA model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.ganilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from fastai.basics import *\n",
    "from typing import List\n",
    "from fastai.vision.gan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from upit.models.junyanz import define_G, define_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the generator that was introduced in the [GANILLA paper](https://arxiv.org/abs/1703.10593)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BasicBlock_Ganilla(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, use_dropout, stride=1):\n",
    "        super(BasicBlock_Ganilla, self).__init__()\n",
    "        self.rp1 = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=0, bias=False)\n",
    "        self.bn1 = nn.InstanceNorm2d(planes)\n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(use_dropout)\n",
    "        self.rp2 = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.InstanceNorm2d(planes)\n",
    "        self.out_planes = planes\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.InstanceNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "            self.final_conv = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(self.expansion * planes * 2, self.expansion * planes, kernel_size=3, stride=1,\n",
    "                                        padding=0, bias=False),\n",
    "                nn.InstanceNorm2d(self.expansion * planes)\n",
    "            )\n",
    "        else:\n",
    "            self.final_conv = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(planes*2, planes, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "                nn.InstanceNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(self.rp1(x))))\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(self.rp2(out)))\n",
    "        inputt = self.shortcut(x)\n",
    "        catted = torch.cat((out, inputt), 1)\n",
    "        out = self.final_conv(catted)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C2_size, C3_size, C4_size, C5_size, fpn_weights, feature_size=128):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        self.sum_weights = fpn_weights #[1.0, 0.5, 0.5, 0.5]\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp1 = nn.ReflectionPad2d(1)\n",
    "        #self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp2 = nn.ReflectionPad2d(1)\n",
    "        #self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp3 = nn.ReflectionPad2d(1)\n",
    "        #self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        self.P2_1 = nn.Conv2d(C2_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P2_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.rp4 = nn.ReflectionPad2d(1)\n",
    "        self.P2_2 = nn.Conv2d(int(feature_size), int(feature_size/2), kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        #self.P1_1 = nn.Conv2d(feature_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        #self.P1_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp5 = nn.ReflectionPad2d(1)\n",
    "        #self.P1_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        C2, C3, C4, C5 = inputs\n",
    "\n",
    "        i = 0\n",
    "        P5_x = self.P5_1(C5) * self.sum_weights[i]\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        #P5_x = self.rp1(P5_x)\n",
    "        # #P5_x = self.P5_2(P5_x)\n",
    "        i += 1\n",
    "        P4_x = self.P4_1(C4) * self.sum_weights[i]\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        #P4_x = self.rp2(P4_x)\n",
    "        # #P4_x = self.P4_2(P4_x)\n",
    "        i += 1\n",
    "        P3_x = self.P3_1(C3) * self.sum_weights[i]\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_upsampled_x = self.P3_upsampled(P3_x)\n",
    "        #P3_x = self.rp3(P3_x)\n",
    "        #P3_x = self.P3_2(P3_x)\n",
    "        i += 1\n",
    "        P2_x = self.P2_1(C2) * self.sum_weights[i]\n",
    "        P2_x = P2_x * self.sum_weights[2] + P3_upsampled_x\n",
    "        P2_upsampled_x = self.P2_upsampled(P2_x)\n",
    "        P2_x = self.rp4(P2_upsampled_x)\n",
    "        P2_x = self.P2_2(P2_x)\n",
    "\n",
    "        return P2_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf, use_dropout, fpn_weights, block, layers):\n",
    "        self.inplanes = ngf\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # first conv\n",
    "        self.pad1 = nn.ReflectionPad2d(input_nc)\n",
    "        self.conv1 = nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=0, bias=True)\n",
    "        self.in1 = nn.InstanceNorm2d(ngf)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pad2 = nn.ReflectionPad2d(1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        # Output layer\n",
    "        self.pad3 = nn.ReflectionPad2d(output_nc)\n",
    "        self.conv2 = nn.Conv2d(64, output_nc, 7)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "        if block == BasicBlock_Ganilla:\n",
    "            # residuals\n",
    "            self.layer1 = self._make_layer_ganilla(block, 64, layers[0], use_dropout, stride=1)\n",
    "            self.layer2 = self._make_layer_ganilla(block, 128, layers[1], use_dropout, stride=2)\n",
    "            self.layer3 = self._make_layer_ganilla(block, 128, layers[2], use_dropout, stride=2)\n",
    "            self.layer4 = self._make_layer_ganilla(block, 256, layers[3], use_dropout, stride=2)\n",
    "\n",
    "            fpn_sizes = [self.layer1[layers[0] - 1].conv2.out_channels,\n",
    "                         self.layer2[layers[1] - 1].conv2.out_channels,\n",
    "                         self.layer3[layers[2] - 1].conv2.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv2.out_channels]\n",
    "\n",
    "        else:\n",
    "            print(\"This block type is not supported\")\n",
    "            sys.exit()\n",
    "\n",
    "        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2], fpn_sizes[3], fpn_weights)\n",
    "\n",
    "    \n",
    "    def _make_layer_ganilla(self, block, planes, blocks, use_dropout, stride=1):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inplanes, planes, use_dropout, stride))\n",
    "            self.inplanes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        img_batch = inputs\n",
    "\n",
    "        x = self.pad1(img_batch)\n",
    "        x = self.conv1(x)\n",
    "        x = self.in1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pad2(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        out = self.fpn([x1, x2, x3, x4]) # use all resnet layers\n",
    "\n",
    "        out = self.pad3(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(net, init_type='normal', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                torch.nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                torch.nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                torch.nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ganilla_generator(input_nc, output_nc, ngf, drop, fpn_weights=[1.0, 1.0, 1.0, 1.0], init_type='normal', gain=0.02, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 GANILLA generator.\"\"\"\n",
    "    model = ResNet(input_nc, output_nc, ngf, drop, fpn_weights, BasicBlock_Ganilla, [2, 2, 2, 2],  **kwargs)\n",
    "    init_weights(model,init_type='normal', gain=0.02)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test for a few things:\n",
    "1. The generator can indeed be initialized correctly\n",
    "2. A random image can be passed into the model successfully with the correct size output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = torch.randn(4,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ganilla_generator(3,3,64,0.5)\n",
    "with torch.no_grad():\n",
    "    out1 = m(img1)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_models.cyclegan.ipynb.\n",
      "Converted 01b_models.junyanz.ipynb.\n",
      "Converted 02_data.unpaired.ipynb.\n",
      "Converted 03_train.cyclegan.ipynb.\n",
      "Converted 04_inference.cyclegan.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_tracking.wandb.ipynb.\n",
      "Converted 07_models.dualgan.ipynb.\n",
      "Converted 08_train.dualgan.ipynb.\n",
      "Converted 09_models.ganilla.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
