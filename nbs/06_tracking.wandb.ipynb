{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases Callback\n",
    "> Defines a fastai Callback for specifically tracking image-to-image translation experiments in Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp tracking.wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-01 20:14:18.055063: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import wandb\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.callback.wandb import _format_metadata, _format_config\n",
    "from fastai.basics import *\n",
    "from fastai.vision.gan import *\n",
    "from upit.models.cyclegan import *\n",
    "from upit.data.unpaired import *\n",
    "from upit.train.cyclegan import *\n",
    "from upit.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SaveModelAtEndCallback(Callback):\n",
    "    def __init__(self, fname='model', with_opt=False): store_attr()\n",
    "    def _save(self, name): self.last_saved_path = self.learn.save(name, with_opt=self.with_opt)\n",
    "    def after_fit(self, **kwargs): self._save(f'{self.fname}')\n",
    "    @property\n",
    "    def name(self): return \"save_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def log_dataset(main_path, folder_names=None, name=None, metadata={}, description='raw dataset'):\n",
    "    \"Log dataset folder\"\n",
    "    # Check if wandb.init has been called in case datasets are logged manually\n",
    "    if wandb.run is None:\n",
    "        raise ValueError('You must call wandb.init() before log_dataset()')\n",
    "    path = Path(main_path)\n",
    "    if not path.is_dir():\n",
    "        raise f'path must be a valid directory: {path}'\n",
    "    name = ifnone(name, path.name)\n",
    "    _format_metadata(metadata)\n",
    "    artifact_dataset = wandb.Artifact(name=name, type='dataset', metadata=metadata, description=description)\n",
    "    # log everything in folder_names\n",
    "    if not folder_names: folder_names = [p.name for p in path.ls() if p.is_dir()]\n",
    "    for p in path.ls():\n",
    "        if p.is_dir():\n",
    "            if p.name in folder_names and p.name != 'models': artifact_dataset.add_dir(str(p.resolve()), name=p.name)\n",
    "        else: artifact_dataset.add_file(str(p.resolve()))\n",
    "    wandb.run.use_artifact(artifact_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UPITWandbCallback(Callback):\n",
    "    \"Saves model topology, losses & metrics\"\n",
    "    remove_on_fetch,order = True,Recorder.order+1\n",
    "    # Record if watch has been called previously (even in another instance)\n",
    "    _wandb_watch_called = False\n",
    "\n",
    "    def __init__(self, log=\"gradients\", log_preds=True, log_model=True, log_dataset=False, folder_names=None, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True):\n",
    "        # Check if wandb.init has been called\n",
    "        if wandb.run is None:\n",
    "            raise ValueError('You must call wandb.init() before WandbCallback()')\n",
    "        # W&B log step\n",
    "        self._wandb_step = wandb.run.step - 1  # -1 except if the run has previously logged data (incremented at each batch)\n",
    "        self._wandb_epoch = 0 if not(wandb.run.step) else math.ceil(wandb.run.summary['epoch']) # continue to next epoch\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Call watch method to log model topology, gradients & weights\"\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n",
    "        if not self.run: return\n",
    "\n",
    "        # Log config parameters\n",
    "        log_config = self.learn.gather_args()\n",
    "        _format_config(log_config)\n",
    "        try:\n",
    "            wandb.config.update(log_config, allow_val_change=True)\n",
    "        except Exception as e:\n",
    "            print(f'WandbCallback could not log config parameters -> {e}')\n",
    "\n",
    "        if not WandbCallback._wandb_watch_called:\n",
    "            WandbCallback._wandb_watch_called = True\n",
    "            # Logs model topology and optionally gradients and weights\n",
    "            wandb.watch(self.learn.model, log=self.log)\n",
    "\n",
    "       \n",
    "        # log dataset\n",
    "        assert isinstance(self.log_dataset, (str, Path, bool)), 'log_dataset must be a path or a boolean'\n",
    "        if self.log_dataset is True:\n",
    "            if Path(self.dls.path) == Path('.'):\n",
    "                print('WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\"')\n",
    "                self.log_dataset = False\n",
    "            else:\n",
    "                self.log_dataset = self.dls.path\n",
    "        \n",
    "        if self.log_dataset:\n",
    "            self.log_dataset = Path(self.log_dataset)\n",
    "            assert self.log_dataset.is_dir(), f'log_dataset must be a valid directory: {self.log_dataset}'\n",
    "            metadata = {'path relative to learner': os.path.relpath(self.log_dataset, self.learn.path)}\n",
    "            if self.folder_names:\n",
    "                assert isinstance(self.folder_names, list), 'folder_names must be a list of folder names as strings'\n",
    "                for name in self.folder_names: assert isinstance(name, str), 'the elements of folder_names must be strings'\n",
    "            log_dataset(main_path=self.log_dataset, folder_names=self.folder_names, name=self.dataset_name, metadata=metadata)\n",
    "\n",
    "\n",
    "        # log model\n",
    "        if self.log_model and not hasattr(self, 'save_model'):\n",
    "            print('Adding SaveModelAtEndCallback()')\n",
    "            self.learn.add_cb(SaveModelAtEndCallback())\n",
    "            self.add_save_model = True\n",
    "        else: self.add_save_model = False\n",
    "\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                if not self.valid_dl:\n",
    "                    if not len(self.dls.valid_ds):\n",
    "                        print('Saving training set predictions')\n",
    "                        #Initializes the batch watched\n",
    "                        wandbRandom = random.Random(self.seed)  # For repeatability\n",
    "                        self.n_preds = min(self.n_preds, len(self.dls.train_ds))\n",
    "                        idxs = wandbRandom.sample(range(len(self.dls.train_ds)), self.n_preds)\n",
    "                        test_items = [getattr(self.dls.train_ds.items, 'iloc', self.dls.train_ds.items)[i] for i in idxs]\n",
    "                        self.preds_dl = self.dls.test_dl(test_items, with_labels=True)\n",
    "                        \n",
    "                else: self.preds_dl = self.valid_dl\n",
    "                self.learn.add_cb(FetchPredsCallback(dl=self.preds_dl, with_input=True, with_decoded=True, reorder=self.reorder))\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                print(f'WandbCallback was not able to prepare a DataLoader for logging prediction samples -> {e}')\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        if self.training:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "\n",
    "            wandb.log({'epoch': self._wandb_epoch, 'train_loss': float(to_detach(self.smooth_loss.clone())), \n",
    "                       'raw_loss': float(to_detach(self.loss.clone())), **hypers}, step=self._wandb_step)\n",
    "\n",
    "    def log_predictions(self, preds):\n",
    "        raise NotImplementedError(\"To be implemented\")\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "        # Correct any epoch rounding error and overwrite value\n",
    "        self._wandb_epoch = round(self._wandb_epoch)\n",
    "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "        # Log sample predictions\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                self.log_predictions(self.learn.fetch_preds.preds)\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                print(f'WandbCallback was not able to get prediction samples -> {e}')\n",
    "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.log_model:\n",
    "            if self.save_model.last_saved_path is None:\n",
    "                print('WandbCallback could not retrieve a model to upload')\n",
    "            else:\n",
    "                metadata = {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}\n",
    "                log_model(self.save_model.last_saved_path, metadata=metadata)\n",
    "        self.run = True\n",
    "        if self.log_preds: self.remove_cb(FetchPredsCallback)\n",
    "        if self.add_save_model: self.remove_cb(SaveModelCallback)\n",
    "        wandb.log({})  # ensure sync of last step\n",
    "        self._wandb_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtmabraham\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-09-01 20:14:23.642203: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">polished-yogurt-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/tmabraham/spell-nbs\" target=\"_blank\">https://wandb.ai/tmabraham/spell-nbs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/tmabraham/spell-nbs/runs/124u5axw\" target=\"_blank\">https://wandb.ai/tmabraham/spell-nbs/runs/124u5axw</a><br/>\n",
       "                Run data is saved locally in <code>/spell/nbs/wandb/run-20210901_201422-124u5axw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/.fastai/data/horse2zebra/trainA)... Done. 0.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/.fastai/data/horse2zebra/trainB)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding SaveModelAtEndCallback()\n",
      "Saving training set predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>id_loss_A</th>\n",
       "      <th>id_loss_B</th>\n",
       "      <th>gen_loss_A</th>\n",
       "      <th>gen_loss_B</th>\n",
       "      <th>cyc_loss_A</th>\n",
       "      <th>cyc_loss_B</th>\n",
       "      <th>D_A_loss</th>\n",
       "      <th>D_B_loss</th>\n",
       "      <th>frechet_inception_distance</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.217916</td>\n",
       "      <td>1.503999</td>\n",
       "      <td>1.580567</td>\n",
       "      <td>0.399754</td>\n",
       "      <td>0.429108</td>\n",
       "      <td>3.183522</td>\n",
       "      <td>3.330419</td>\n",
       "      <td>0.378100</td>\n",
       "      <td>0.381198</td>\n",
       "      <td>91.221733</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.887602</td>\n",
       "      <td>1.201838</td>\n",
       "      <td>1.206192</td>\n",
       "      <td>0.297459</td>\n",
       "      <td>0.318193</td>\n",
       "      <td>2.533871</td>\n",
       "      <td>2.611986</td>\n",
       "      <td>0.253656</td>\n",
       "      <td>0.270657</td>\n",
       "      <td>90.616170</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandbCallback was not able to get prediction samples -> To be implemented\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3195<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 107.96MB of 107.96MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/spell/nbs/wandb/run-20210901_201422-124u5axw/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/spell/nbs/wandb/run-20210901_201422-124u5axw/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>2</td></tr><tr><td>train_loss</td><td>8.8876</td></tr><tr><td>raw_loss</td><td>6.48146</td></tr><tr><td>wd_0</td><td>0.01</td></tr><tr><td>sqr_mom_0</td><td>0.999</td></tr><tr><td>lr_0</td><td>1e-05</td></tr><tr><td>mom_0</td><td>0.5</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>_runtime</td><td>303</td></tr><tr><td>_timestamp</td><td>1630527565</td></tr><tr><td>_step</td><td>49</td></tr><tr><td>id_loss_A</td><td>1.20184</td></tr><tr><td>id_loss_B</td><td>1.20619</td></tr><tr><td>gen_loss_A</td><td>0.29746</td></tr><tr><td>gen_loss_B</td><td>0.31819</td></tr><tr><td>cyc_loss_A</td><td>2.53387</td></tr><tr><td>cyc_loss_B</td><td>2.61199</td></tr><tr><td>D_A_loss</td><td>0.25366</td></tr><tr><td>D_B_loss</td><td>0.27066</td></tr><tr><td>frechet_inception_distance</td><td>90.61617</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>raw_loss</td><td>█▆▅▄▄▃▄▄▃▃▂▄▃▂▃▃▃▂▃▄▃▂▃▃▂▂▂▂▂▂▄▂▂▂▁▂▂▁▂▁</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>██████████████████████▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁</td></tr><tr><td>mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>id_loss_A</td><td>█▁</td></tr><tr><td>id_loss_B</td><td>█▁</td></tr><tr><td>gen_loss_A</td><td>█▁</td></tr><tr><td>gen_loss_B</td><td>█▁</td></tr><tr><td>cyc_loss_A</td><td>█▁</td></tr><tr><td>cyc_loss_B</td><td>█▁</td></tr><tr><td>D_A_loss</td><td>█▁</td></tr><tr><td>D_B_loss</td><td>█▁</td></tr><tr><td>frechet_inception_distance</td><td>█▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">polished-yogurt-6</strong>: <a href=\"https://wandb.ai/tmabraham/spell-nbs/runs/124u5axw\" target=\"_blank\">https://wandb.ai/tmabraham/spell-nbs/runs/124u5axw</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cuda\n",
    "import tempfile\n",
    "\n",
    "horse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n",
    "folders = horse2zebra.ls().sorted()\n",
    "trainA_path = folders[2]\n",
    "trainB_path = folders[3]\n",
    "testA_path = folders[0]\n",
    "testB_path = folders[1]\n",
    "dls = get_dls(trainA_path, trainB_path, num_A=100, num_B=100, load_size=286)\n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'dryrun' # run offline\n",
    "wandb.init()\n",
    "cycle_gan = CycleGAN(3,3,64)\n",
    "learn = cycle_learner(dls, cycle_gan,opt_func=partial(Adam,mom=0.5,sqr_mom=0.999),\n",
    "                    metrics=[FrechetInceptionDistance()],\n",
    "                    cbs=[UPITWandbCallback(log_preds=True, log_model=True, log_dataset=horse2zebra, folder_names=[trainA_path.name,trainB_path.name])])\n",
    "\n",
    "learn.fit_flat_lin(1,1,2e-4)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_models.cyclegan.ipynb.\n",
      "Converted 01b_models.junyanz.ipynb.\n",
      "Converted 02_data.unpaired.ipynb.\n",
      "Converted 03_train.cyclegan.ipynb.\n",
      "Converted 04_inference.cyclegan.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_tracking.wandb.ipynb.\n",
      "Converted 07_models.dualgan.ipynb.\n",
      "Converted 08_train.dualgan.ipynb.\n",
      "Converted 09_models.ganilla.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
